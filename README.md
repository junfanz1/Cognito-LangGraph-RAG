
<!-- TOC --><a name="cognito-langgraph-rag"></a>
# Cognito LangGraph RAG

<!-- TOC --><a name="star-history"></a>
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=junfanz1/Cognito-LangGraph-RAG&type=Date)](https://star-history.com/#junfanz1/Cognito-LangGraph-RAG&Date)

<!-- TOC --><a name="content"></a>
## Content

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

   * [1. Purpose of the Project](#1-purpose-of-the-project)
   * [2. Input and Output](#2-input-and-output)
   * [3. LLM Technology Stack](#3-llm-technology-stack)
   * [4. Challenges and Difficulties](#4-challenges-and-difficulties)
   * [5. Future Business Impact and Further Improvements](#5-future-business-impact-and-further-improvements)
   * [6. Target Audience and Benefits](#6-target-audience-and-benefits)
   * [7. Advantages and Disadvantages](#7-advantages-and-disadvantages)
   * [8. Tradeoffs](#8-tradeoffs)
   * [9. Highlight and Summary](#9-highlight-and-summary)
   * [10. Future Enhancements](#10-future-enhancements)
   * [11. Prerequisites](#11-prerequisites)
   * [12. Setup](#12-setup)
   * [13. Code Explanation](#13-code-explanation)
      + [`graph/graph.py`](#graphgraphpy)
      + [`graph/state.py`](#graphstatepy)
      + [`graph/nodes/retrieve.py`](#graphnodesretrievepy)
      + [`graph/nodes/grade_documents.py`](#graphnodesgrade_documentspy)
      + [`graph/nodes/web_search.py`](#graphnodesweb_searchpy)
      + [`graph/nodes/generate.py`](#graphnodesgeneratepy)
      + [`ingestion.py`](#ingestionpy)
   * [15. Crucial Functions](#15-crucial-functions)
      + [`grade_documents(state: GraphState)`](#grade_documentsstate-graphstate)
      + [`decide_to_generate(state)`](#decide_to_generatestate)
      + [`retrieve(state: GraphState)`](#retrievestate-graphstate)
   * [16. Future Improvements](#16-future-improvements)
   * [17. Additional Information](#17-additional-information)
   * [Acknowledgements](#acknowledgements)

<!-- TOC end -->


<!-- TOC --><a name="1-purpose-of-the-project"></a>
## 1. Purpose of the Project

This project implements an advanced Retrieval Augmented Generation (RAG) workflow to enhance question-answering accuracy and reduce LLM hallucinations. It leverages LangGraph to create a stateful, multi-step process that includes document retrieval, relevance grading, and web search fallback.

<!-- TOC --><a name="2-input-and-output"></a>
## 2. Input and Output

**Input:** Natural language questions from users.


- a list of URLs

```python
urls = [
    "https://lilianweng.github.io/posts/2024-11-28-reward-hacking/",
    "https://lilianweng.github.io/posts/2024-07-07-hallucination/",
    "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/",
    "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
]
```

- Question:

```python
what is agent memory?
```


**Output:** Coherent and contextually relevant responses generated by the LLM, augmented with retrieved documents and/or web search results.

<img src="https://github.com/user-attachments/assets/145936ec-ae91-4d1a-85a2-e545ff17f88d" width="40%" height="40%">

```python
Hello Advanced RAG!
            +-----------+       
            | __start__ |       
            +-----------+       
                  *             
                  *             
                  *             
            +----------+        
            | retrieve |        
            +----------+        
                  *             
                  *             
                  *             
         +-----------------+    
         | grade_documents |    
         +-----------------+    
           ...         ...      
          .               .     
        ..                 ...  
+-----------+                 . 
| websearch |              ...  
+-----------+             .     
           ***         ...      
              *       .         
               **   ..          
            +----------+        
            | generate |        
            +----------+        
                  *             
                  *             
                  *             
             +---------+        
             | __end__ |        
             +---------+        
%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
	__start__([<p>__start__</p>]):::first
	retrieve(retrieve)
	grade_documents(grade_documents)
	websearch(websearch)
	generate(generate)
	__end__([<p>__end__</p>]):::last
	__start__ --> retrieve;
	generate --> __end__;
	retrieve --> grade_documents;
	websearch --> generate;
	grade_documents -.-> websearch;
	grade_documents -.-> generate;
	classDef default fill:#f2f0ff,line-height:1.2
	classDef first fill-opacity:0
	classDef last fill:#bfb6fc

---Retrieving---
---check document relevance to question---
---grade: document not relevant---
---grade: document not relevant---
---grade: document not relevant---
---grade: document not relevant---
---assess granted documents---
---decision: not all docs are relevant to question ---
---web search---
---generate---
{'question': 'what is agent memory?', 'generation': 'Agent memory is a crucial component that allows AI to store and recall information across interactions. It enables AI models to retain information rather than treating every query as a new conversation. Different types of memory are used in AI agents.', 'web_search': True, 'documents': [Document(metadata={}, page_content='In general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan\nUnderstanding Agent Memory in AI: Types, Use Cases, and Implementation | by Siladitya Ghosh | Feb, 2025 | Medium Understanding Agent Memory in AI: Types, Use Cases, and Implementation The secret lies in agent memory — a crucial component that allows AI to store and recall information across interactions. What agent memory is and why it’s essential The different types of memory used in AI agents Real-world use cases of memory in AI systems By the end, you’ll have a solid understanding of how AI agents use memory and how to implement it in your own projects. What is Agent Memory? Agent memory allows AI models to retain information across interactions rather than treating every query as a new conversation. Types of Agent Memory\nEnroll now: https://bit.ly/3YwWJeR Build agentic memory into your applications with LLMs as Operating Systems: Agent Memory, a short course')]}

Process finished with exit code 0

```

LangSmith trace: https://smith.langchain.com/public/9653a9cd-927d-4223-8db5-45d20dff1112/r

![image](https://github.com/user-attachments/assets/71f89b84-c681-4328-bcbc-91b7307bffd3)

Because my provided URL documents don't have necessary information to answer this question, so the agent went for web_search to find the answer.


- `question`: `what is agent memory?`
- `generation`: `Agent memory is a crucial component that allows AI to store and recall information across interactions. It enables AI models to retain information rather than treating every query as a new conversation. Different types of memory are used in AI agents.`
- `web_search`: true

Documents

`In general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan
Understanding Agent Memory in AI: Types, Use Cases, and Implementation | by Siladitya Ghosh | Feb, 2025 | Medium Understanding Agent Memory in AI: Types, Use Cases, and Implementation The secret lies in agent memory — a crucial component that allows AI to store and recall information across interactions. What agent memory is and why it’s essential The different types of memory used in AI agents Real-world use cases of memory in AI systems By the end, you’ll have a solid understanding of how AI agents use memory and how to implement it in your own projects. What is Agent Memory? Agent memory allows AI models to retain information across interactions rather than treating every query as a new conversation. Types of Agent Memory
Enroll now: https://bit.ly/3YwWJeR Build agentic memory into your applications with LLMs as Operating Systems: Agent Memory, a short course`

<!-- TOC --><a name="3-llm-technology-stack"></a>
## 3. LLM Technology Stack

-   **LangChain:**
    - Used for building LLM applications by providing abstractions for LLMs, vector stores, and other components.
    - Facilitates the creation of chains and agents.
-   **LangGraph:**
    - A framework for building stateful, multi-actor applications with LLMs.
    - Enables the creation of complex workflows with conditional logic and state management.
-   **OpenAI:**
    - Provides powerful LLMs (e.g., GPT-3.5, GPT-4) for generating responses and embeddings.
-   **ChromaDB:**
    - A vector database for storing and retrieving document embeddings.
    - `Chroma.from_documents` is used to create and persist the vectorstore.
    - `Chroma.as_retriever()` is used to create a retriever object.
-   **Tavily Search:**
    - A search API for retrieving web search results.
    - `TavilySearchResults` tool is used to execute web searches.
-   **Python:**
-   **dotenv:**
    - Used for managing environment variables, such as API keys, securely.

<!-- TOC --><a name="4-challenges-and-difficulties"></a>
## 4. Challenges and Difficulties

-  Ensuring document relevance to user queries.
-  Reducing LLM hallucinations.
-  Orchestrating complex workflows with multiple LLM calls.
-  Integrating web search seamlessly.
-   Managing state within the LangGraph workflow.
-   Efficiently grading retrieved documents.

<!-- TOC --><a name="5-future-business-impact-and-further-improvements"></a>
## 5. Future Business Impact and Further Improvements

-  Improved customer support systems.
-  Enhanced knowledge management platforms.
-  More accurate and reliable chatbots.
-  Automated content generation.
-   Future Improvements:
    -   Implementing more sophisticated document grading algorithms.
    -   Adding more diverse data sources.
    -   Improving web search result processing.
    -   Adding user feedback loops to refine the system.
    -   Adding more advanced memory management.

<!-- TOC --><a name="6-target-audience-and-benefits"></a>
## 6. Target Audience and Benefits

-  Developers and researchers interested in advanced RAG systems.
-  Businesses seeking to improve information retrieval and question-answering capabilities.
-  Benefits:
    -  Improved accuracy and reliability of LLM responses.
    -  Reduced hallucinations.
    -  Enhanced information retrieval.
    -   Modular and extensible architecture.

<!-- TOC --><a name="7-advantages-and-disadvantages"></a>
## 7. Advantages and Disadvantages

-   **Advantages:**
    - Robust workflow with document grading and web search fallback.
    -  Modular and extensible design.
    -  Improved accuracy and reliability.
-   **Disadvantages:**
    -  Increased complexity compared to simpler RAG systems.
    -  Potential latency due to multiple LLM calls and web searches.
    -  Relies on external API's which can have cost associated.

<!-- TOC --><a name="8-tradeoffs"></a>
## 8. Tradeoffs

-   **Complexity vs. Accuracy:** The advanced workflow adds complexity but improves accuracy.
-   **Latency vs. Thoroughness:** Web search and document grading increase latency but ensure more thorough results.
-   **Cost vs. Performance:** Utilizing LLM API's and search tools incur cost, but improve performance.

<!-- TOC --><a name="9-highlight-and-summary"></a>
## 9. Highlight and Summary

The core of this project is to build a robust and reliable question-answering system using Retrieval Augmented Generation, but with a focus on mitigating common LLM challenges like hallucinations and irrelevant responses. We've implemented an advanced RAG pipeline that goes beyond basic retrieval. 

First, we leverage LangGraph to orchestrate a stateful workflow, which allows us to manage complex interactions between different components. This is crucial because we're not just retrieving and generating; we're also grading the relevance of retrieved documents. The key innovation here is the 'grade_documents' node. We use an LLM to assess the semantic relevance of each retrieved document to the user's query. If a document is deemed irrelevant, we trigger a web search using Tavily Search as a fallback, ensuring that the final response is comprehensive and accurate. This conditional logic, managed by LangGraph, is what distinguishes this system. 

We've also focused on modularity. Each step—retrieval, grading, web search, and generation—is encapsulated in its own node, making the system highly extensible. We're using ChromaDB for efficient vector storage and retrieval, and OpenAI's LLMs for both embedding and generation. 

In essence, this project tackles the 'garbage in, garbage out' problem by actively filtering and augmenting the retrieved context. The LangGraph framework allows us to create a pipeline that dynamically adapts to the quality of retrieved information, resulting in more reliable and accurate responses compared to a basic RAG setup. This design shows an understanding of how to build complex LLM applications that address real-world challenges."

<!-- TOC --><a name="10-future-enhancements"></a>
## 10. Future Enhancements

-  Implement more sophisticated document grading algorithms.
-  Add more diverse data sources.
-  Improve web search result processing.
-  Add user feedback loops to refine the system.
-   Implement long term memory.
-   Implement dynamic tool selection.

<!-- TOC --><a name="11-prerequisites"></a>
## 11. Prerequisites

- Python 3.7+
-  OpenAI API key
-  Tavily API key
-   Install required packages: `pip install -r requirements.txt`

<!-- TOC --><a name="12-setup"></a>
## 12. Setup

1.  Clone the repository.
2.  Install the required packages: `pip install -r requirements.txt`
3.  Create a `.env` file and add your OpenAI and Tavily API keys:

```
OPENAI_API_KEY=
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=
LANGCHAIN_TRACING_V2=true
TAVILY_API_KEY=
PYTHONPATH=/Users/junfanzhu/Desktop/langgraph
```

4.  Run the main application: `python graph/graph.py`



<!-- TOC --><a name="13-code-explanation"></a>
## 13. Code Explanation

<!-- TOC --><a name="graphgraphpy"></a>
### `graph/graph.py`

-   **Purpose:** Orchestrates the entire RAG workflow using LangGraph.
-   **Detailed Explanation:**
    -   Loads environment variables using `load_dotenv()`.
    -   Initializes a `StateGraph` with `GraphState`.
    -   Defines nodes: `RETRIEVE`, `GRADE_DOCUMENTS`, `WEBSEARCH`, `GENERATE`.
    -   Sets the entry point to `RETRIEVE`.
    -   Adds edges to connect nodes:
        -   `RETRIEVE` -> `GRADE_DOCUMENTS`.
        -   `GRADE_DOCUMENTS` -> `WEBSEARCH` or `GENERATE` (conditional).
        -   `WEBSEARCH` -> `GENERATE`.
        -   `GENERATE` -> `END`.
    -   `decide_to_generate(state)` function:
        -   Checks `state["web_search"]` to determine if web search is needed.
        -   Returns `WEBSEARCH` or `GENERATE` based on the condition.
    -   Compiles the graph using `workflow.compile()`.
    -   Invokes the compiled graph with a sample question.
    -   Generates a visualization of the graph using `get_graph().draw_mermaid_png()`.

<!-- TOC --><a name="graphstatepy"></a>
### `graph/state.py`

-   **Purpose:** Defines the `GraphState` TypedDict to manage the state of the workflow.
-   **Detailed Explanation:**
    -   `GraphState` includes:
        -   `question`: The user's question.
        -   `generation`: The LLM-generated response.
        -   `web_search`: A boolean flag indicating whether web search is needed.
        -   `documents`: A list of retrieved documents.
    -   TypedDict ensures type safety and clarity in state management.

<!-- TOC --><a name="graphnodesretrievepy"></a>
### `graph/nodes/retrieve.py`

-   **Purpose:** Retrieves relevant documents from ChromaDB.
-   **Detailed Explanation:**
    -   Takes the `GraphState` as input.
    -   Extracts the user's question from `state["question"]`.
    -   Invokes the ChromaDB retriever using `retriever.invoke(question)`.
    -   Returns a dictionary containing the retrieved documents and the question.

<!-- TOC --><a name="graphnodesgrade_documentspy"></a>
### `graph/nodes/grade_documents.py`

-   **Purpose:** Grades the retrieved documents for relevance using an LLM.
-   **Detailed Explanation:**
    -   Takes the `GraphState` as input.
    -   Extracts the question and documents from the state.
    -   Iterates through each document:
        -   Invokes the `retrieval_grader` chain with the document and question.
        -   Checks the `binary_score` from the grader's output.
        -   If the score is "yes", appends the document to `filtered_docs`.
        -   If the score is "no", sets `web_search` to `True`.
    -   Returns a dictionary containing the filtered documents, the question, and the `web_search` flag.
    -   `retrieval_grader` uses a structured output parser, and a defined prompt to make the grading decision.

<!-- TOC --><a name="graphnodesweb_searchpy"></a>
### `graph/nodes/web_search.py`

-   **Purpose:** Performs a web search using Tavily Search.
-   **Detailed Explanation:**
    -   Takes the `GraphState` as input.
    -   Extracts the question and documents from the state.
    -   Invokes the `web_search_tool` with the question.
    -   Joins the content of the search results into a single string.
    -   Creates a `Document` object with the joined content.
    -   Appends the web search results to the existing documents or creates a new list if no documents exist.
    -   Returns a dictionary containing the updated documents and the question.

<!-- TOC --><a name="graphnodesgeneratepy"></a>
### `graph/nodes/generate.py`

-   **Purpose:** Generates a response based on the retrieved and processed documents.
-   **Detailed Explanation:**
    -   Takes the `GraphState` as input.
    -   Extracts the question and documents from the state.
    -   Invokes the `generation_chain` with the documents and question.
    -   Returns a dictionary containing the documents, question, and generated response.
    -   `generation_chain` uses a prompt from langchain hub, and the OpenAI LLM to generate the output.

<!-- TOC --><a name="ingestionpy"></a>
### `ingestion.py`

-   **Purpose:** Loads, chunks, embeds, and stores documents in ChromaDB.
-   **Detailed Explanation:**
    -   Loads documents from specified URLs using `WebBaseLoader`.
    -   Splits documents into chunks using `RecursiveCharacterTextSplitter`.
    -   Embeds documents using `OpenAIEmbeddings`.
    -   Stores embeddings in ChromaDB using `Chroma.from_documents`.
    -   Creates a retriever from ChromaDB using `Chroma.as_retriever()`.

<!-- TOC --><a name="15-crucial-functions"></a>
## 15. Crucial Functions

<!-- TOC --><a name="grade_documentsstate-graphstate"></a>
### `grade_documents(state: GraphState)`

-   **Detailed Elaboration:**
    -   This function is crucial because it acts as a quality control mechanism for the retrieved documents.
    -   It uses an LLM, specifically `retrieval_grader`, to determine the relevance of each retrieved document to the user's question.
    -   The `retrieval_grader` chain uses a structured LLM output, ensuring that the grading result is in a consistent format (`binary_score`: "yes" or "no").
    -   By iterating through each document and grading it, the function filters out irrelevant documents, preventing the LLM from generating responses based on incorrect context.
    -   The `web_search` flag is set to `True` if any document is deemed irrelevant, triggering a web search to provide more comprehensive information.
    -   This function directly impacts the quality and accuracy of the final generated response, reducing hallucinations and improving user satisfaction.

<!-- TOC --><a name="decide_to_generatestate"></a>
### `decide_to_generate(state)`

-   **Detailed Elaboration:**
    -   This function acts as a conditional router within the LangGraph workflow.
    -   It examines the `web_search` flag within the `GraphState` to determine the next step in the workflow.
    -   If `web_search` is `True`, it routes the workflow to the `WEBSEARCH` node, ensuring that a web search is performed to supplement the retrieved documents.
    -   If `web_search` is `False`, it routes the workflow to the `GENERATE` node, bypassing the web search and directly generating the response from the retrieved documents.
    -   This conditional routing is essential for creating a dynamic and adaptive workflow that can handle varying levels of document relevance. It allows the system to make intelligent decisions based on the quality of the retrieved information, ensuring that the final response is always as accurate and comprehensive as possible.

<!-- TOC --><a name="retrievestate-graphstate"></a>
### `retrieve(state: GraphState)`

-   **Detailed Elaboration:**
    -   This function is the entry point for retrieving relevant documents from the knowledge base.
    -   It takes the user's question from the `GraphState` and uses the ChromaDB retriever to perform a semantic search.
    -   The retriever leverages document embeddings to find documents that are semantically similar to the query, even if they don't contain the exact keywords.
    -   By returning the retrieved documents and the original question, this function sets the stage for the subsequent steps in the workflow, such as grading and generation.
    -   The efficiency and accuracy of this retrieval step are critical for the overall performance of the RAG system, as it determines the initial set of documents that will

<!-- TOC --><a name="16-future-improvements"></a>
## 16. Future Improvements

-   Implement long term memory.
-   Implement dynamic tool selection.
-   Add more sophisticated document grading algorithms.
-   Add more diverse data sources.
-   Improve web search result processing.
-   Add user feedback loops to refine the system.

<!-- TOC --><a name="17-additional-information"></a>
## 17. Additional Information

-   The `graph.png` file visualizes the workflow.
-   The `.env` file should be kept secure and not committed to version control.
-   Unit tests are provided to ensure the correctness of the code.
-   This project uses LangGraph to manage state, which allows for complex, multi-step workflows.

<!-- TOC --><a name="acknowledgements"></a>
## Acknowledgements

[Eden Marco: LangGraph-Develop LLM powered AI agents with LangGraph](https://www.udemy.com/course/langgraph)
